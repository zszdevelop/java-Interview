# 计算大数据量，频繁操作数据库优化

## 1. 背景介绍

我们有个业务场景是需要计算几万个用户的特征，这些用户特征分布在10个表中（不单单是获取某个字段，拿到记录后还需要手动计算）。

**最初的版本**

1. 循环出这几万个用户信息
2. 根据用户信息到对应的10张表中查询特征值，并计算
3. 将这n个特征值合并
4. 保存到数据库中

在数据量小的时候，该方案并没有什么问题，几分钟就完事了。但我们线上环境需要计算几万，甚至几十万的时候，时间成倍增长（我们同步3万个用户，花费了15个小时）。

## 2. 优化方案1

尝试使用线程池，将3万个数据划分成每100个用户一组添加到线程池。

>线程池配置：核心线程20个，最大线程40个

但效果并不好，我们这里的操作主要是频繁操作数据库，这一部分耗费的时间过长。

>每个用户查询10张关联表，在保存。
>
>3万个用户，实际查询就需要30万次，保存3万次

## 3. 优化方案2

关于这种频繁操作数据库，io密集型的操作。我们的优化方式思路：

- 减少请求数量
- 减少请求大小

关于请求数量上，我们查询一个用户的时间和查询100个用户的时间是差不多的，我们把计算放在内存中操作，最后再将这些数据组合就会快很多。

实现思路

1. 将3万个用户，每100个划分为一组，添加进线程池
2. 一次查询出线程中的100个用户的关联数据
3. 分别计算这100个用户特征，然后将特征值放入特征map中（key 为用户id，value对应的特征值）
4. 将特征集的n个map，通过用户id查询组合成新特征集
5. 批量保存100个特征集

优化后3万个用户同步花费时间为5分钟

